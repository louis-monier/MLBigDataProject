{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course project - Machine Learning on Big Data \n",
    "***\n",
    "In the following, we perform various implementations of gradient descent from [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf) by Sebastian Ruder (2017)\n",
    "\n",
    "In this project, our main objective is to solve a **linear regression** problem by minimizing the Mean Square Loss (MSE) between the model predictions $h_{\\omega}(x^{(i)})$ and the ground truth $y^{(i)}$ :\n",
    "\n",
    "$$ \\mathcal{J}_{MSE} = \\dfrac{1}{n} \\sum_{i=1}^{n} ( h_{\\omega}(x^{(i)}) - y^{(i)})^{2}$$\n",
    "\n",
    "where : \n",
    "* $\\mathcal{D} = \\{x^{(i)} ; y^{(i)}\\}_{i=1}^{n}$ is the training dataset\n",
    "* $\\omega$ are the weights of the model $\\in \\mathbb{R}^{1xd}$\n",
    "* $h_{\\omega}$ is a linear approximator : $h_{\\omega}(x^{(i)}) = \\sum_{i=1}^{d} \\omega_i x_{j}^{(i)} = \\omega^{T} x_{j}$\n",
    "#### <font color=blue>  Students : Louis Monier & Vincent Gouteux </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import findspark \n",
    "#findspark.init() \n",
    "import pyspark\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Course Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Data preparation\n",
    "***\n",
    "* We first used a simple toy dataset to perform linear regression and easily check our implementations. Concretely, we generate n data points sampled from a Gaussian distribution $\\mathcal{N}(0,1)$. Noise is added so optimization techniques do not converge too fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 1000 # number of examples\n",
    "d = 2 # number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Number of examples :  1000\n",
       "Number of features :  2\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bias = np.ones((n,1)) \n",
    "X = np.random.normal(loc=0.0, scale=1.0, size=(n,1)) # generate n points from Gaussian distribution\n",
    "noise = np.random.normal(loc=0.0, scale=1.0, size=(n,1)) # add noise \n",
    "\n",
    "y = 5 * X + 2 + noise # simple toy dataset\n",
    "X = np.hstack((bias, X)) \n",
    "data = np.hstack((X, y))\n",
    "w_star = np.dot(np.linalg.pinv(X), y).T\n",
    "print(\"Number of examples : \", n)\n",
    "print(\"Number of features : \", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Warm up with Vanilla Gradient Descent (aka Batch Gradient Descent)\n",
    "\n",
    "Vanilla Gradient Descent (Vanilla GD) computes the gradient of the loss w.r.t. to the parameters θ for the entire training dataset:\n",
    "\n",
    "$$ \\omega = \\omega - \\eta . \\nabla{\\omega} \\mathcal{J}(\\omega) $$\n",
    "\n",
    "We mention once and for all (here in the case of Vanilla GD), the derivation of the gardient : \n",
    "$$ \\nabla{\\omega} \\mathcal{J}_{MSE} = \\dfrac{2}{n} \\sum_{i=1}^{n} ( h_{\\omega}(x^{(i)}) - y^{(i)}) . x^{(i)} $$\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">* Start training..\n",
       "Iter : [0/200] ; MSE = 3.368\n",
       "Iter : [20/200] ; MSE = 2.202\n",
       "Iter : [40/200] ; MSE = 1.440\n",
       "Iter : [60/200] ; MSE = 0.941\n",
       "Iter : [80/200] ; MSE = 0.616\n",
       "Iter : [100/200] ; MSE = 0.403\n",
       "Iter : [120/200] ; MSE = 0.264\n",
       "Iter : [140/200] ; MSE = 0.173\n",
       "Iter : [160/200] ; MSE = 0.113\n",
       "Iter : [180/200] ; MSE = 0.074\n",
       "* End training..\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.randn(1,d)\n",
    "history_GD = []\n",
    "eta = 1e-2 # step-size \n",
    "nb_iter = 200\n",
    "\n",
    "print(\"* Start training..\")\n",
    "for i in range(nb_iter):\n",
    "    rdd = sc.parallelize(data)\n",
    "    rdd = rdd.map(lambda x : 2 * ( np.dot(w, x[:-1]) - x[-1] ) * x[:-1])\n",
    "    rdd = rdd.reduce(lambda x,y : (x+y)) / n\n",
    "    w -= eta * rdd\n",
    "\n",
    "    mse = np.linalg.norm(w - w_star)\n",
    "    history_GD.append(mse)\n",
    "    \n",
    "    if (i%20 == 0) :\n",
    "        print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(i, nb_iter, mse))\n",
    "print(\"* End training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(history_GD)\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.title(\"Vanilla Gradient Descent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Mini-batch Gradient Descent\n",
    "***\n",
    "Mini-batch gradient descent performs an update for every mini-batch of n training examples:\n",
    "\n",
    "\n",
    "$$ \\omega = \\omega - \\eta . \\nabla{\\omega} \\mathcal{J}(\\omega ; x^{(i:i+n)} ; y^{(i:i+n)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Batch size : 50\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PARTITION WE ARE GOING TO USE FOR ALL GD METHOD\n",
    "\n",
    "nb_repart = 20 # number of batch  \n",
    "assert(n > nb_repart)\n",
    "batch_size = n // nb_repart\n",
    "print(\"Batch size :\", batch_size)\n",
    "nb_iter = 10\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "rdd = rdd.repartition(nb_repart).cache() # divide the data in \"nb_repart\" number of partitions\n",
    "rdd = rdd.glom().zipWithIndex() # coalescing all elements within each partition into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">* Start training..\n",
       "Iter : [0/10] ; MSE = 3.446\n",
       "Iter : [1/10] ; MSE = 2.240\n",
       "Iter : [2/10] ; MSE = 1.456\n",
       "Iter : [3/10] ; MSE = 0.948\n",
       "Iter : [4/10] ; MSE = 0.618\n",
       "Iter : [5/10] ; MSE = 0.403\n",
       "Iter : [6/10] ; MSE = 0.264\n",
       "Iter : [7/10] ; MSE = 0.174\n",
       "Iter : [8/10] ; MSE = 0.115\n",
       "Iter : [9/10] ; MSE = 0.077\n",
       "* End training..\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.randn(1,d)\n",
    "history_BGD = []\n",
    "eta = 1e-2\n",
    "\n",
    "    \n",
    "print(\"* Start training..\")\n",
    "for i in range(nb_iter):\n",
    "    for j in range(nb_repart):\n",
    "        batch = rdd.filter(lambda x: x[1] == j)\n",
    "        batch = batch.flatMap(lambda x: x[0])\n",
    "        n_in_batch = batch.count()\n",
    "        if (n_in_batch > 0):\n",
    "            batch = batch.map(lambda x: 2 * ( np.dot(w, x[:-1]) - x[-1] ) * x[:-1])\n",
    "            batch = batch.reduce(lambda a,b: (a+b)) / n_in_batch\n",
    "            w -= eta* batch\n",
    "            mse = np.linalg.norm(w - w_star)\n",
    "            history_BGD.append(mse)\n",
    "        else:\n",
    "            print(\"Empty RDD..\")\n",
    "\n",
    "    print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(i, nb_iter, mse)) \n",
    "print(\"* End training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(history_BGD)\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Mini-batch Gradient Descent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Stochastic Gradient Descent (SGD)\n",
    "***\n",
    "We tried several methods to perform SGD. The first one was to create as many repartitions as points in the dataset. We observed the repartitions were not uniform which resulted in a lot of of empty useless RDDs. Moreover, even if we dealt with empty RDDs, the running time was very long, which is the opposite of what SGD was designed for. \n",
    "\n",
    "We then decided to move on and implement SGD using some tricks. This approach does not exploit as much the benefits of the Map Reduce framework but it works properly. As many optimizers in the following are based on SGD, we decided to keep this version to have a stable, well-performing reference and show relevant comparisons. To be as thorough as possible, we also implement all the following gradient descent variants based on classic gradient descent.\n",
    "\n",
    "We recall the gradient update rule for SGD which consists to uniformaly-at-random select an example from the dataset $\\{x^{(i)} ; y^{(i)}\\}$ (instead of the entire dataset for Vanilla GD or a subset of the dataset for Batch GD) and evaluate the gradient : \n",
    "\n",
    "$$ \\omega = \\omega - \\eta . \\nabla{\\omega} \\mathcal{J}(\\omega ; x^{(i)} ; y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Batch size : 50\n",
       "* Start training..\n",
       "Iter : [0/20] ; MSE = 1.603\n",
       "Iter : [4/20] ; MSE = 0.169\n",
       "Iter : [8/20] ; MSE = 0.116\n",
       "Iter : [12/20] ; MSE = 0.137\n",
       "Iter : [16/20] ; MSE = 0.138\n",
       "* End training..\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.randn(1,d)\n",
    "history_SGD = []\n",
    "eta = 1e-2\n",
    "print(\"Batch size :\", batch_size)\n",
    "nb_iter = 10\n",
    "print(\"* Start training..\")\n",
    "for j in range(nb_repart):\n",
    "    batch = rdd.filter(lambda x: x[1] == j)\n",
    "    batch = batch.flatMap(lambda x: x[0])\n",
    "    n_in_batch = batch.count()\n",
    "    if (n_in_batch > 0):\n",
    "      for k,r in enumerate(batch.take(n_in_batch)) : \n",
    "        x = r[:2]\n",
    "        y = r[2:]\n",
    "        grad = 2*x*(np.dot(w,x) - y )\n",
    "        w = w - eta*grad\n",
    "        mse = np.linalg.norm(w-w_star)\n",
    "        history_SGD.append(mse)\n",
    "    else:\n",
    "        print(\"Empty RDD..\")\n",
    "\n",
    "    if (j%4 == 0) :\n",
    "      print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \n",
    "print(\"* End training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(history_GD, label = 'GD')\n",
    "plt.plot(history_BGD, label = 'BGD')\n",
    "plt.plot(history_SGD[:200], label = 'SGD')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.title('Summary/comparison of different gradient descent methods')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we sums up pros and cons of basic gradient methods seen above (based on the [article](https://arxiv.org/pdf/1609.04747.pdf)) : \n",
    "* Vanilla GD can be very slow and is intractable for datasets that do not fit in memory. Here we used a toy training set so the method works and show good convergence.\n",
    "* Vanilla GD is redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update\n",
    "* SGD does away with this redundancy by performing one update at a time. It is much faster than vanilla GD and can be used online. However, SGD has a much high variance.\n",
    "* Mini-batch GD takes the best of both worlds. It is a trade-off between vanilla GD and SGD. \n",
    "\n",
    "Now, let's review some more sophisticated gradient descent optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Momentum\n",
    "***\n",
    "SGD is subject to oscillations during training. Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. The method consists of adding a fraction $\\gamma$ of the update vector of the past time step to the current update vector :\n",
    "\n",
    "$$ v_t = \\gamma v_{t-1} + \\eta  \\nabla{\\omega} \\mathcal{J}(\\omega) $$\n",
    "$$ \\omega = \\omega - v_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Batch size : 50\n",
       "* Start training..\n",
       "Iter : [0/20] ; MSE = 0.535\n",
       "Iter : [4/20] ; MSE = 0.084\n",
       "Iter : [8/20] ; MSE = 0.242\n",
       "Iter : [12/20] ; MSE = 0.331\n",
       "Iter : [16/20] ; MSE = 0.165\n",
       "* End training..\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.zeros((1,d))\n",
    "history_momentum = []\n",
    "v_prev = 0. # initialization\n",
    "w = np.random.randn(1,d)\n",
    "eta = 1e-2\n",
    "gamma = 0.8\n",
    "print(\"Batch size :\", batch_size)   \n",
    "print(\"* Start training..\")\n",
    "for j in range(nb_repart):\n",
    "    batch = rdd.filter(lambda x: x[1] == j)\n",
    "    batch = batch.flatMap(lambda x: x[0])\n",
    "    n_in_batch = batch.count()\n",
    "    if (n_in_batch > 0):\n",
    "      for k,r in enumerate(batch.take(n_in_batch)) : \n",
    "        x = r[:2]\n",
    "        y = r[2:]\n",
    "        grad = 2*x*(np.dot(w,x) - y )\n",
    "        if (k == 0) :\n",
    "          vlast = 0\n",
    "          v = eta*grad\n",
    "        else :\n",
    "          v = gamma*vlast + eta*grad\n",
    "          vlast = v\n",
    "        w = w - v\n",
    "        if (i%10 == 0 ) :\n",
    "          print(w)\n",
    "        mse = np.linalg.norm(w-w_star)\n",
    "        history_momentum.append(mse)\n",
    "    else:\n",
    "        print(\"Empty RDD..\")\n",
    "    if (j%4 == 0) :\n",
    "      print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \n",
    "print(\"* End training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(history_SGD[:200], label = 'SGD')\n",
    "plt.plot(history_momentum[:200], label = 'SGD Momentum')\n",
    "\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.title('Comparison MSE SGD vs GD Momentum')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the gamma parameter seems to play an important role in the momentum gradient descent, we decided to perform hyperparameter tuning to estimate optimal range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Gamma =  0.0\n",
       "Gamma =  0.1\n",
       "Gamma =  0.2\n",
       "Gamma =  0.30000000000000004\n",
       "Gamma =  0.4\n",
       "Gamma =  0.5\n",
       "Gamma =  0.6000000000000001\n",
       "Gamma =  0.7000000000000001\n",
       "Gamma =  0.8\n",
       "Gamma =  0.9\n",
       "10000\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### MOMENTUM ### #finding best gamma\n",
    "history_gamma = []\n",
    "for j in range (10): \n",
    "    gamma = 0.1*j\n",
    "    w = np.zeros(2)\n",
    "    step = 0.02 \n",
    "    print(\"Gamma = \", gamma)\n",
    "    for j in range(nb_repart):\n",
    "        batch = rdd.filter(lambda x: x[1] == j)\n",
    "        batch = batch.flatMap(lambda x: x[0])\n",
    "        n_in_batch = batch.count()\n",
    "        if (n_in_batch > 0):\n",
    "          for k,r in enumerate(batch.take(n_in_batch)) : \n",
    "            x = r[:2]\n",
    "            y = r[2:]\n",
    "            grad = 2*x*(np.dot(w,x) - y )\n",
    "            if (k == 0) :\n",
    "              vlast = 0\n",
    "              v = eta*grad\n",
    "            else :\n",
    "              v = gamma*vlast + eta*grad\n",
    "              vlast = v\n",
    "            w = w - v\n",
    "            if (i%10 == 0 ) :\n",
    "              print(w)\n",
    "            mse = np.linalg.norm(w-w_star)\n",
    "            history_gamma.append(mse)\n",
    "        else:\n",
    "            print(\"Empty RDD..\")\n",
    "print(len(history_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(history_gamma[:200], label = 'Gamma = 0.0')\n",
    "plt.plot(history_gamma[1000:1200], label = 'Gamma = 0.1')\n",
    "plt.plot(history_gamma[2000:2200], label = 'Gamma = 0.2')\n",
    "plt.plot(history_gamma[3000:3200], label = 'Gamma = 0.3')\n",
    "plt.plot(history_gamma[4000:4200], label = 'Gamma = 0.4')\n",
    "plt.plot(history_gamma[5000:5200], label = 'Gamma = 0.5')\n",
    "plt.plot(history_gamma[6000:6200], label = 'Gamma = 0.6')\n",
    "plt.plot(history_gamma[7000:7200], label = 'Gamma = 0.7')\n",
    "plt.plot(history_gamma[8000:8200], label = 'Gamma = 0.8')\n",
    "plt.plot(history_gamma[9000:9200], label = 'Gamma = 0.9')\n",
    "plt.title('MSE for different gradient descent momentum gammas')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 -  Nesterov accelerated gradient (NAG)\n",
    "***\n",
    "NAG is a way to give our momentum term this kind of prescience. We know that we will use our momentum term γvt−1 to move the parameters $\\theta$. Computing $\\theta -  \\gamma v_{t-1}$ thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be :\n",
    "\n",
    "$$ v_t = \\gamma v_{t-1} + \\eta . \\nabla{\\omega} \\mathcal{J}(\\omega - \\gamma v_{t-1}) $$\n",
    "$$ \\omega = \\omega - v_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Batch size : 50\n",
       "* Start training..\n",
       "Iter : [0/20] ; MSE = 0.786\n",
       "Iter : [4/20] ; MSE = 0.254\n",
       "Iter : [8/20] ; MSE = 0.298\n",
       "Iter : [12/20] ; MSE = 0.295\n",
       "Iter : [16/20] ; MSE = 0.244\n",
       "* End training..\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.zeros((1,d))\n",
    "history_nesterov = []\n",
    "v_prev = 0. # initialization\n",
    "eta = 1e-2\n",
    "gamma = 0.8\n",
    "print(\"Batch size :\", batch_size)\n",
    "print(\"* Start training..\")\n",
    "for j in range(nb_repart):\n",
    "    batch = rdd.filter(lambda x: x[1] == j)\n",
    "    batch = batch.flatMap(lambda x: x[0])\n",
    "    n_in_batch = batch.count()\n",
    "    if (n_in_batch > 0):\n",
    "      for k,r in enumerate(batch.take(n_in_batch)) : \n",
    "        x = r[:2]\n",
    "        y = r[2:]\n",
    "        if (i == 0) :\n",
    "          vlast = 0\n",
    "          grad = 2*x*(np.dot(w,x) - y )\n",
    "          v = step*grad\n",
    "        else :\n",
    "          grad = 2*x*(np.dot(w-v,x) - y )\n",
    "          v = gamma*vlast + step*grad\n",
    "          vlast = v\n",
    "        w = w - v\n",
    "        if (i%10 == 0 ) :\n",
    "          print(w)\n",
    "        mse = np.linalg.norm(w-w_star)\n",
    "        #print(mse)\n",
    "        history_nesterov.append(mse)\n",
    "    else:\n",
    "        print(\"Empty RDD..\")\n",
    "    if (j%4 == 0) :\n",
    "      print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \n",
    "print(\"* End training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(history_GD, label = 'SGD ')\n",
    "plt.plot(history_momentum[:200], label = 'SGD Momentum')\n",
    "plt.plot(history_nesterov[:200], label = 'SGD Nesterov ')\n",
    "\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.title('Comparison MSE SGD vs GD Momentum')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Adagrad\n",
    "***\n",
    "Adagrad adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data. The gradient update rule is given by the following expression :\n",
    "\n",
    "$$ \\omega_{t+1} = \\omega_{t} - \\dfrac{\\eta}{\\sqrt(G_T + \\epsilon)} \\odot g_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Batch size : 50\n",
       "* Start training..\n",
       "Iter : [0/20] ; MSE = 0.355\n",
       "Iter : [4/20] ; MSE = 0.183\n",
       "Iter : [8/20] ; MSE = 0.108\n",
       "Iter : [12/20] ; MSE = 0.154\n",
       "Iter : [16/20] ; MSE = 0.139\n",
       "* End training..\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.zeros((1,d))\n",
    "history_adagrad = []\n",
    "sum_grad = np.zeros((1,d))\n",
    "eta = 0.7\n",
    "gamma = 0.9 # classic value\n",
    "nb_iter = 200\n",
    "epsilon = 1e-8\n",
    "print(\"Batch size :\", batch_size)\n",
    "print(\"* Start training..\")\n",
    "for j in range(nb_repart):\n",
    "    batch = rdd.filter(lambda x: x[1] == j)\n",
    "    batch = batch.flatMap(lambda x: x[0])\n",
    "    n_in_batch = batch.count()\n",
    "    if (n_in_batch > 0):\n",
    "      for k,r in enumerate(batch.take(n_in_batch)) : \n",
    "        x = r[:2]\n",
    "        y = r[2:]\n",
    "        grad = 2*x*(np.dot(w,x) - y )\n",
    "        sum_grad += grad**2\n",
    "        adjusted_eta = eta / np.sqrt(epsilon + sum_grad)\n",
    "        w -= np.multiply(adjusted_eta, grad) # element-wise \n",
    "    \n",
    "        mse = np.linalg.norm(w - w_star)\n",
    "        history_adagrad.append(mse)  \n",
    "        if (i%10 == 0 ) :\n",
    "          print(w)\n",
    "    else:\n",
    "        print(\"Empty RDD..\")\n",
    "    if (j%4 == 0) :\n",
    "      print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \n",
    "print(\"* End training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(history_GD, label = 'SGD ')\n",
    "plt.plot(history_momentum[:200], label = 'SGD Momentum')\n",
    "plt.plot(history_nesterov[:200], label = 'SGD Nesterov ')\n",
    "plt.plot(history_adagrad[:200], label = 'SGD Adagrad ')\n",
    "\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.title('Comparison')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad is working fine, however we noticed that we needed a step much larger than for momentum and Nesterov. Changing the step size $\\eta$ makes the comparison between methods harder. However, if we select such value for Adagrad, the curve is smoother and we reach approximately the same convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 -  Adadelta\n",
    "***\n",
    "It is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients : the sum of gradients is recursively defined as a decaying average of all past squared gradients. \n",
    "\n",
    "$$ \\omega_{t+1} = \\omega_{t} - \\dfrac{RMS[\\Delta \\omega]_{t+1}}{RMS[g]_{t}} g_t $$\n",
    "$$ \\omega_{t+1} = \\omega_{t} + \\Delta \\omega_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Batch size : 50\n",
       "* Start training..\n",
       "Iter : [0/20] ; MSE = 4.410\n",
       "Iter : [4/20] ; MSE = 0.133\n",
       "Iter : [8/20] ; MSE = 0.137\n",
       "Iter : [12/20] ; MSE = 0.242\n",
       "Iter : [16/20] ; MSE = 0.187\n",
       "* End training..\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.zeros((1,d))\n",
    "history_adadelta = []\n",
    "delta = np.zeros((1,d))\n",
    "Eg_prev = 0.\n",
    "Ed_prev = 0.\n",
    "RMSd = 0.\n",
    "\n",
    "eta = 3e-3\n",
    "gamma = 0.9 # classic value\n",
    "nb_iter = 200\n",
    "epsilon = 1e-8\n",
    "\n",
    "print(\"Batch size :\", batch_size)\n",
    "\n",
    "print(\"* Start training..\")\n",
    "for j in range(nb_repart):\n",
    "    batch = rdd.filter(lambda x: x[1] == j)\n",
    "    batch = batch.flatMap(lambda x: x[0])\n",
    "    n_in_batch = batch.count()\n",
    "    if (n_in_batch > 0):\n",
    "      for k,r in enumerate(batch.take(n_in_batch)) : \n",
    "        x = r[:2]\n",
    "        y = r[2:]\n",
    "        grad = 2*x*(np.dot(w,x) - y ) /n\n",
    "        \n",
    "        Eg = gamma * Eg_prev + (1-gamma) * grad**2\n",
    "        RMSg = np.sqrt(Eg  + epsilon)\n",
    "\n",
    "        w -= (RMSd / RMSg) * grad\n",
    "\n",
    "        ###\n",
    "        delta -= (eta / RMSg) * grad\n",
    "        Ed = gamma * Eg_prev + (1-gamma) * delta**2\n",
    "        RMSd = np.sqrt(Ed  + epsilon)\n",
    "\n",
    "        Eg_prev = Eg\n",
    "        Ed_prev = Ed\n",
    "\n",
    "        mse = np.linalg.norm(w - w_star)\n",
    "        history_adadelta.append(mse)\n",
    "    else:\n",
    "        print(\"Empty RDD..\")\n",
    "    if (j%4 == 0):\n",
    "      print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \n",
    "print(\"* End training..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(history_adadelta[:200])\n",
    "\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.title('Adadelta');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - RMSprop\n",
    "***\n",
    "RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients :\n",
    "\n",
    "$$ E[{g^2}]_t = 0.9 E[{g^2}]_{t-1} + 0.1 g_{t}^2 $$\n",
    "$$ \\omega_{t+1} = \\omega_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">* Start training..\n",
       "Iter : [0/20] ; MSE = 3.994\n",
       "Iter : [4/20] ; MSE = 0.262\n",
       "Iter : [8/20] ; MSE = 0.126\n",
       "Iter : [12/20] ; MSE = 0.183\n",
       "Iter : [16/20] ; MSE = 0.151\n",
       "* End training..\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.zeros((1,d))\n",
    "history_RMS = []\n",
    "delta = np.zeros((1,d))\n",
    "Eg_prev = 0.\n",
    "Ed_prev = 0.\n",
    "\n",
    "eta = 3e-2\n",
    "gamma = 0.9 # classic value\n",
    "nb_iter = 200\n",
    "epsilon = 1e-8\n",
    "\n",
    "print(\"* Start training..\")\n",
    "for j in range(nb_repart):\n",
    "  batch = rdd.filter(lambda x: x[1] == j)\n",
    "  batch = batch.flatMap(lambda x: x[0])\n",
    "  n_in_batch = batch.count()\n",
    "  if (n_in_batch > 0):\n",
    "    for k,r in enumerate(batch.take(n_in_batch)) : \n",
    "      x = r[:2]\n",
    "      y = r[2:]\n",
    "      grad = 2*x*(np.dot(w,x) - y )/n\n",
    "\n",
    "      ###\n",
    "      Eg = gamma * Eg_prev + (1-gamma) * grad**2\n",
    "      RMSg = np.sqrt(Eg  + epsilon)\n",
    "\n",
    "      w -= (eta / RMSg) * grad\n",
    "\n",
    "      Eg_prev = Eg\n",
    "\n",
    "      mse = np.linalg.norm(w - w_star)\n",
    "      history_RMS.append(mse)\n",
    "  if (j%4 == 0): \n",
    "    print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \n",
    "\n",
    "print(\"* End training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(history_adadelta[:300], label = 'Adadelta')\n",
    "plt.plot(history_RMS[:300], label = ' RMSProp')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.title('RMSprop');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Adaptive Moment Estimation (Adam)\n",
    "***\n",
    "Another method that computes adaptive learning rates for each parameter :\n",
    "\n",
    "$$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$\n",
    "$$ v_t = \\beta_2 v_{t-1} +(1 - \\beta_2) g_t^2 $$\n",
    "$$ \\omega_{t+1} = \\omega_t - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">* Start training..\n",
       "Iter : [0/20] ; MSE = 2.138\n",
       "Iter : [4/20] ; MSE = 0.252\n",
       "Iter : [8/20] ; MSE = 0.155\n",
       "Iter : [12/20] ; MSE = 0.104\n",
       "Iter : [16/20] ; MSE = 0.099\n",
       "* End training..\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.zeros((1,d))\n",
    "history_adam = []\n",
    "delta = np.zeros((1,d))\n",
    "mom_prev = 0.\n",
    "v_prev = 0.\n",
    "\n",
    "eta = 5e-3\n",
    "nb_iter = 200\n",
    "epsilon = 1e-8\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "print(\"* Start training..\")\n",
    "for j in range(nb_repart):\n",
    "  batch = rdd.filter(lambda x: x[1] == j)\n",
    "  batch = batch.flatMap(lambda x: x[0])\n",
    "  n_in_batch = batch.count()\n",
    "  if (n_in_batch > 0):\n",
    "    for k,r in enumerate(batch.take(n_in_batch)) : \n",
    "      x = r[:2]\n",
    "      y = r[2:]\n",
    "      grad = 2*x*(np.dot(w,x) - y )/n\n",
    "      mom = beta1 * mom_prev + (1-beta1) * grad \n",
    "      v = beta2 * v_prev + (1-beta2) * grad**2\n",
    "\n",
    "      avg_mom = mom / (1 - beta1)\n",
    "      avg_v = v / (1 - beta1)\n",
    "\n",
    "      w -= (eta / np.sqrt(avg_v)) * avg_mom\n",
    "\n",
    "      mse = np.linalg.norm(w - w_star)\n",
    "      history_adam.append(mse)\n",
    "      \n",
    "      v_prev = v\n",
    "      mom_prev = mom \n",
    "  if (j%4 == 0 ):\n",
    "    print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse)) \n",
    "\n",
    "print(\"* End training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(history_adam[:300], label = 'Adam')\n",
    "plt.plot(history_adadelta[:300], label = 'Adadelta')\n",
    "plt.plot(history_RMS[:300], label = 'RMSProp')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.title('Adam');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - AdaMax\n",
    "***\n",
    "It is a generalization of the $v_t$ update with the $l_{\\infty}$ norm :\n",
    "\n",
    "\n",
    "$$ u_t = \\beta_2^{\\infty} v_{t-1} +(1 - \\beta_2^{\\infty}) \\mid g_t\\mid^{\\infty} = max(\\beta_2 . v_{t-1}, \\mid g_t \\mid) $$\n",
    "$$ \\omega_{t+1} = \\omega_t - \\dfrac{\\eta}{u_t} \\hat{m}_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">* Start training..\n",
       "Iter : [0/20] ; MSE = 2.912\n",
       "Iter : [4/20] ; MSE = 0.271\n",
       "Iter : [8/20] ; MSE = 0.368\n",
       "Iter : [12/20] ; MSE = 0.085\n",
       "Iter : [16/20] ; MSE = 0.289\n",
       "* End training..\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.zeros((1,d))\n",
    "history_adamax = []\n",
    "delta = np.zeros((1,d))\n",
    "mom_prev = 0.\n",
    "u_prev = 0.\n",
    "\n",
    "eta = 1e-3\n",
    "nb_iter = 200\n",
    "epsilon = 1e-8\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "print(\"* Start training..\")\n",
    "for j in range(nb_repart):\n",
    "  batch = rdd.filter(lambda x: x[1] == j)\n",
    "  batch = batch.flatMap(lambda x: x[0])\n",
    "  n_in_batch = batch.count()\n",
    "  if (n_in_batch > 0):\n",
    "    for k,r in enumerate(batch.take(n_in_batch)) : \n",
    "      x = r[:2]\n",
    "      y = r[2:]\n",
    "      grad = 2*x*(np.dot(w,x) - y )/n\n",
    "\n",
    "      mom = beta1 * mom_prev + (1-beta1) * grad \n",
    "      v = beta2 * v_prev + (1-beta2) * grad**2\n",
    "\n",
    "      avg_mom = mom / (1 - beta1)\n",
    "      avg_v = v / (1 - beta1)\n",
    "      u = np.maximum(beta2 * v_prev, np.abs(grad))\n",
    "\n",
    "      avg_mom = mom / (1 - beta1)\n",
    "      avg_v = v / (1 - beta1)\n",
    "      w -= (eta / u) * avg_mom\n",
    "      \n",
    "      v_prev = v\n",
    "      mom_prev = mom \n",
    "\n",
    "      mse = np.linalg.norm(w - w_star)\n",
    "      history_adamax.append(mse)\n",
    "  if ( j%4 == 0):\n",
    "    print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse))\n",
    "print(\"* End training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(history_adamax[:300], label = 'Adamax')\n",
    "plt.plot(history_adam[:300], label = 'Adam')\n",
    "plt.plot(history_adadelta[:300], label = 'Adadelta')\n",
    "plt.plot(history_RMS[:300], label = 'RMSProp')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Mean Square Error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - Nadam \n",
    "***\n",
    "This is a combination of Adam and Nestorov :\n",
    "\n",
    "$$ \\Delta \\omega_t = - \\dfrac{RMS[\\Delta \\omega]_{t-1}}{RMS[g]_t} g_t $$\n",
    "$$ \\omega_{t+1} = \\omega_t + \\Delta \\omega_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">* Start training..\n",
       "Iter : [0/20] ; MSE = 0.734\n",
       "Iter : [4/20] ; MSE = 0.127\n",
       "Iter : [8/20] ; MSE = 0.229\n",
       "Iter : [12/20] ; MSE = 0.409\n",
       "Iter : [16/20] ; MSE = 0.131\n",
       "* End training..\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.zeros((1,d))\n",
    "history_nadam = []\n",
    "delta = np.zeros((1,d))\n",
    "mom_prev = 0.\n",
    "u_prev = 0.\n",
    "\n",
    "eta = 5e-2\n",
    "nb_iter = 200\n",
    "epsilon = 1e-8\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "print(\"* Start training..\")\n",
    "for j in range(nb_repart):\n",
    "  batch = rdd.filter(lambda x: x[1] == j)\n",
    "  batch = batch.flatMap(lambda x: x[0])\n",
    "  n_in_batch = batch.count()\n",
    "  if (n_in_batch > 0):\n",
    "    for k,r in enumerate(batch.take(n_in_batch)) : \n",
    "      x = r[:2]\n",
    "      y = r[2:]\n",
    "      grad = 2*x*(np.dot(w,x) - y )/n\n",
    "\n",
    "      mom = beta1 * mom_prev + (1-beta1) * grad \n",
    "      v = beta2 * v_prev + (1-beta2) * grad**2\n",
    "\n",
    "      avg_mom = mom / (1 - beta1)\n",
    "      avg_v = v / (1 - beta1)\n",
    "      u = np.maximum(beta2 * v_prev, np.abs(grad))\n",
    "\n",
    "      avg_mom = mom / (1 - beta1)\n",
    "      avg_v = v / (1 - beta1)\n",
    "      w -= (eta / (np.sqrt(avg_v)+ epsilon)) * (beta1 * avg_mom + ((1-beta1)* grad)/(1-beta1))\n",
    "      \n",
    "      v_prev = v\n",
    "      mom_prev = mom \n",
    "\n",
    "      mse = np.linalg.norm(w - w_star)\n",
    "      history_nadam.append(mse)\n",
    "  if (j%4 == 0):\n",
    "    print(\"Iter : [{}/{}] ; MSE = {:.3f}\".format(j, nb_repart, mse))\n",
    "print(\"* End training..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(11, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(history_nadam[:300], label = 'Nadam')\n",
    "plt.plot(history_adamax[:300], label = 'Adamax')\n",
    "plt.plot(history_adam[:300], label = 'Adam')\n",
    "plt.plot(history_adadelta[:300], label = 'Adadelta')\n",
    "plt.plot(history_RMS[:300], label = 'RMSProp')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Mean Square Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(15, 11), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(history_GD, label = 'GD')\n",
    "plt.plot(history_BGD, label = 'BGD')\n",
    "plt.plot(history_SGD[:200], label = 'SGD')\n",
    "plt.plot(history_momentum[:200], label = 'Momentum')\n",
    "plt.plot(history_nesterov[:200], label = 'Nesterov')\n",
    "plt.plot(history_adagrad[:200], label = 'Adagrad')\n",
    "plt.plot(history_adadelta[:200], label = 'Adadelta')\n",
    "plt.plot(history_RMS[:200], label = 'RMS')\n",
    "plt.plot(history_adam[:200], label = 'Adam')\n",
    "plt.plot(history_adamax[:200], label = 'Adamax')\n",
    "plt.plot(history_nadam[:200], label = 'Nadam')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of gradient updates\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "name": "Projet_MLBG_GDImplementations",
  "notebookId": 2692408934725147
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
